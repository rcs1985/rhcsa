kubekubPre-requisite:
    
#yum remove -y podman-docker  

K8S Cluster Install:
    
Kubernetes Cluster Setup Installation - 1Master, 2WorkerNodes:
 
 Note:  All the below commands should be executed in all 3 Nodes:   

# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6q

10.0.7.1    master master          (take IP from "ip a" command and place it here)
10.0.7.2    node1 worker1          (take IP from "ip a" command and place it here)
10.0.7.3    node2 worker2          (take IP from "ip a" command and place it here)

#swapoff -a (to permanently disable swap, make entry on /etc/fstab)

#systemctl stop firewalld ; systemctl disable firewalld; systemctl status firewalld

#setenforce 0

#yum install wget -y 

#wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.6.10-3.1.el7.x86_64.rpm

#yum localinstall containerd.io-1.6.10-3.1.el7.x86_64.rpm -y

#systemctl start  containerd.service; systemctl enable containerd.service; systemctl status containerd.service

#cat<<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# modprobe overlay; sudo modprobe br_netfilter



#cat<<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

#sudo sysctl --system
      
cat<<EOF| sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
      
      
      
      


#sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config


#yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

#systemctl enable --now kubelet

Note:  The below command for Pre-Flight - It will show us if the previous commands configured properly:  This should also be executed 
            in all 3 nodes

#kubeadm init phase preflight

Note: IF there is any error in above command, follow below steps

[root@master ~]# kubeadm init phase preflight
I0621 07:18:50.195909    5747 version.go:256] remote version is much newer: v1.30.2; falling back to: stable-1.29
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2024-06-21T07:18:50Z" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

Note:  The above is known error:  To fix, please follow the below commands:

#rm /etc/containerd/config.toml

#systemctl restart containerd.service

Now preflight ...

#kubeadm init phase preflight


-=======================================================================================================
Should be run only in Master Node:
========================================================================================================

master#kubeadm init --pod-network-cidr=192.168.0.0/24    --> Please note that the output of this command should be saved in a file on master node:

.....

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:192.168.0.60  master1.example.com  master1
192.168.0.59  master1.example.com  master2
192.168.0.61  node2.example.com node2
192.168.0.62  node3.example.com node3


  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.128.0.10:6443 --token x5x7o7.25dctlxi91sg6jqn --discovery-token-ca-cert-hash sha256:eb69c97473f2f7b0de6286230449b8f94ed92dd9d55c02e3c7453e1

=============

master#export KUBECONFIG=/etc/kubernetes/admin.conf


[root@master ~]# kubectl get node
NAME                   STATUS     ROLES           AGE    VERSION
master   NotReady   control-plane   2m5s   v1.25.4
[root@master ~]#

[root@master ~]# kubectl get pod -A
NAMESPACE     NAME                                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-565d847f94-9ffg6                       0/1     Pending   0          4m33s
kube-system   coredns-565d847f94-hfk2t                       0/1     Pending   0          4m33s



=======


wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

=======

[root@master ~]# vim kube-flannel.yml      -->> Change this file with below subnet (which is already we have used while init command)
      "Network": "192.168.0.0/24",       ====> change here for our requried pod network
      "Backend": {
        "Type": "vxlan"
      }
    }
========

[root@master ~]# kubectl create -f kube-flannel.yml
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
[root@master ~]#



[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0          30s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0          16m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0          16m

===

Now Master is ready:
[root@master ~]# kubectl get node
NAME                   STATUS   ROLES           AGE   VERSION
master  Ready    control-plane   17m   v1.25.4
[root@master ~]#


Joining worker node1: 
===============
In Worker1 node:  -> "take the below command from your earlier kubeadm init ... output"
===============
[root@node1 ~]# kubeadm join 10.0.7.1:6443 --token xf3lrl.fb7cniz8139fhvwx --discovery-token-ca-cert-hash sha256:1abb0f47f36b68ba0ce1e2373ab30515f87e936e2f334feb643aeee1ed0529f2

Error in Flannel:

[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS      AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0             3m43s
kube-flannel   kube-flannel-ds-rtfwv                          0/1     Error     3 (44s ago)   88s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0             19m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0             19m

Note: To fix the error, use below patch command:

Note: The below command should be done only on Master node:

[root@master ~]# kubectl patch node worker1 -p '{"spec":{"podCIDR":"192.168.0.0/24"}}'
node/node1 patched
[root@master ~]#


[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS        AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0               8m44s
kube-flannel   kube-flannel-ds-rtfwv                          1/1     Running   6 (2m56s ago)   6m29s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0               24m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0               24m




Now joining worker2 node:
 

[root@node2 mnt]# rm /etc/containerd/config.toml
rm: remove regular file '/etc/containerd/config.toml'? y
[root@node2 mnt]# systemctl restart containerd.service
[root@node2 mnt]#


[root@node2 mnt]# kubeadm join 10.0.7.1:6443 --token xf3lrl.fb7cniz8139fhvwx --discovery-token-ca-cert-hash sha256:1abb0f47f36b68ba0ce1e2373ab30515f87e936e2f334feb643aeee1ed0529f2


[root@master ~]# kubectl get node
NAME                    STATUS   ROLES           AGE   VERSION
master    Ready    control-plane   32m   v1.25.4
node1   Ready    <none>          13m   v1.25.4
node2   Ready    <none>          65s   v1.25.4
[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS      AGE
kube-flannel   kube-flannel-ds-66rzn                          0/1     Error     3 (37s ago)   91s
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0             16m


This needs to be patched to fix flannel error:

[root@master ~]# kubectl patch node node2 -p '{"spec":{"podCIDR":"192.168.0.0/24"}}'
node/node2 patched
[root@master ~]#

Any issues on joining node - use below command to recreate:
#kubeadm token create --print-join-command

----------------------------------------------------
if the server rebooted, swap settings will get revert (if not disabled permanently). nodes wont get start automatically if swap enabled

Perform below steps to start the nodes

On all the nodes
------------------------------
swapoff -a
systemctl stop kubelet
stystemctl start kubelet

export KUBECONFIG=/etc/kubernetes/admin.conf (only on master node)

kebectl get nodes (on all the nodes, this should start the nodes)

20Aug24:

Yaml Basics:
    
1. Dictionary words
Name: Murali
Location: Chennai
2. List
Fruits:
    - Apple
    - Orange
    - Banana
    
Address:
    -  No.10, Ist MainRoad,
       Shashtri Nagar
       Adyar, Tvm Road, Chennai
    - 12, Ist Cross Street,
       Post Office Street,
       K.K.Nagar, Chennai
    -  10, 2nd street,
       Postal and Telegraph Nagar
       Madurai

3. Parent/Child Relationship:
    
    #useradd user300

        user:

        name: rjp

        state: present

    -- its wrong---

     

               user:
                    name: rjpuser
                    state: present
                    
    Pod Creation:
        
    [root@master ~]# kubectl run myweb2 --image=httpd --dry-run=client -o yaml > pod.yml
    [root@master ~]# cat pod.yml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: myweb2
  name: myweb2
spec:
  containers:
  - image: httpd
    name: myweb2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
[root@master ~]# 

[root@master ~]# kubectl create -f pod.yml
[root@master ~]# kubectl get pod 
[root@master ~]#  kubectl get pod -o wide 
[root@master ~]#  kubectl describe pod myweb2

----
ReplicaSet:
    
[root@master ~]# vm replicaset.yml 
apiVersion: apps/v1
kind: ReplicaSet
metadata: 
  name: myapps-rs
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - image: nginx
        name: mypod
  replicas: 4
  selector:
    matchLabels:
      type: front-end
----save---

$kubectl create -f replicaset.yml
$kubectl get pods -o wide
edit the "replicas: 4" on above yaml file and try apply the same
$kubectl apply -f replicaset.yml 
$kubectl get pod -o wide

---
Deployment:
    
    Ensure that no resources are available in your namespace 
    [root@master ~]# kubectl get pod 
    --------nothing should be running here------
[root@master ~]# vim deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: nginx
        name: mycont
  replicas: 4 deI
  selector:
    matchLabels:
      type: front-end

-------
[root@master ~]#kubectl create -f deployment.yml
[root@master ~]#kubectl get pod -o wide -w 

Apply a change now on the file:

[root@master ~]# vim deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: httpd  --> applied
        name: mycont
  replicas: 4
  selector:
    matchLabels:
      type: front-end
---
[root@master ~]#kubectl apply -f deployment.yml
[root@master ~]#kubectl get pod -o wide -w
---Able to see the deployment happenning based on the stratergy -----
Do similar changes/increase replica count and dirt your hands.
-------------------
21Aug24:
NameSpace: (project in openshift)
    
    #kubectl create ns mytcs --dry-run=client -o yaml > ns.yml
    #cat ns.yml    ---> just verify the yaml file
    #kubectl create -f ns.yml
    #kubectl get ns  --> will give you all other avlbl namespaces 
    #kubectl get ns mytcs -o yaml
    #kubectl get all
    #kubectl get pod -n default  -> to check the resources in other namespaces 
    How to change namespace?
    #kubectl config set-context $(kubectl config current-context) --namespace=mytcs     
    
    Go to the other namespace and try create resources:
    #kubectl config set-context $(kubectl config current-context) --namespace=default
    #kubectl create -f replicaset.yml
    #kubectl  get pod 
    #kubectl get pod -n mytcs 
    
    To check the resources in all other namespaces:
    #kubectl  get pod -A
    
    To check the current Namespace we are in:
        kubectl config get-contexts
    
    Service:
    

        $ kubectl create deploy nginxsvc  - -image=nginx --dry-run=client -o yaml > nginxsvc.yml

    $ kubectl create –f nginxsvc.yml 

 List all the objects using 'kubectl get all' 
    $ kubectl get all 

 Expose the service
    $ kubectl expose deploy nginxsvc --port=80  
    $kubectl describe svc nginxsvc  
Name:              nginxsvc 
Namespace:         default 
Labels:            app=nginxsvc 
Annotations:       <none> 
Selector:          app=nginxsvc 
Type:              ClusterIP 
IP Family Policy:  SingleStack 
IP Families:       IPv4 
IP:                10.96.24.47 
IPs:               10.96.24.47 
Port:              <unset>  80/TCP 
TargetPort:        80/TCP 
Endpoints:         10.244.1.3:80,10.244.1.4:80,10.244.2.3:80 
Session Affinity:  None 
Events:            <none> 
$  
 $ kubectl get all 

 Get the yaml output of the service. 
 
$ kubectl get svc nginxsvc -o yaml  
apiVersion: v1 
kind: Service 
metadata: 
  creationTimestamp: "2023-11-16T09:33:08Z" 
  labels: 
    app: nginxsvc 
  name: nginxsvc 
  namespace: default 
  resourceVersion: "39621" 
  uid: a76667f4-b058-44d4-bd02-796ad6d23278 
spec: 
  clusterIP: 10.96.24.47 
  clusterIPs: 
  - 10.96.24.47 
  internalTrafficPolicy: Cluster 
  ipFamilies: 
  - IPv4 
  ipFamilyPolicy: SingleStack 
  ports: 
  - port: 80 
    protocol: TCP 
    targetPort: 80 
  selector: 
    app: nginxsvc 
  sessionAffinity: None 
  type: ClusterIP 
status: 
  loadBalancer: {} 
$  

 Gather the endpoint and service IP. 
$ kubectl get endpoints  
NAME         ENDPOINTS                                   AGE 
kubernetes   172.18.0.4:6443                             14h 
nginxsvc     10.244.1.3:80,10.244.1.4:80,10.244.2.3:80   2m41s 
$  
$ kubectl get svc  
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE 
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   14h 
nginxsvc     ClusterIP   10.96.24.47   <none>        80/TCP    2m51s 
$  

Edit the service and update the type as NodePort. After editing the contents should be similar to next step
$ kubectl edit svc nginxsvc  
service/nginxsvc edited 

Get the yaml form of service configuration. Additions highlighted in bold. 
$ kubectl get svc nginxsvc -o yaml  
apiVersion: v1 
kind: Service 
metadata: 
  creationTimestamp: "2023-11-16T09:33:08Z" 
  labels: 
    app: nginxsvc 
  name: nginxsvc 
  namespace: default 
  resourceVersion: "40350" 
  uid: a76667f4-b058-44d4-bd02-796ad6d23278 
spec: 
  clusterIP: 10.96.24.47 
  clusterIPs: 
  - 10.96.24.47 
  externalTrafficPolicy: Cluster 
  internalTrafficPolicy: Cluster 
  ipFamilies: 
  - IPv4 
  ipFamilyPolicy: SingleStack 
  ports: 
  - **nodePort: 32000** 
    port: 80 
    protocol: TCP 
    targetPort: 80 
  selector: 
    app: nginxsvc 
  sessionAffinity: None 
  **type: NodePort** 
status: 
  loadBalancer: {} 

 Find the node in which application pod is running. 
$ kubectl get pod -o wide  
NAME                        READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES 
nginxsvc-6f45cc47b4-2qcbb   1/1     Running   0          8m51s   10.244.2.3   kind-worker2   <none>           <none> 
nginxsvc-6f45cc47b4-6cw7z   1/1     Running   0          9m18s   10.244.1.3   kind-worker    <none>           <none> 
nginxsvc-6f45cc47b4-l7x4k   1/1     Running   0          8m51s   10.244.1.4   kind-worker    <none>           <none> 
$  

Access the application using node IP and nodePort. 
$ curl http://172.18.0.3:32000 
<!DOCTYPE html> 
<html> 
<head> 
<title>Welcome to nginx!</title> 
<style> 
html { color-scheme: light dark; } 
body { width: 35em; margin: 0 auto; 
font-family: Tahoma, Verdana, Arial, sans-serif; } 
</style> 
</head> 
<body> 
<h1>Welcome to nginx!</h1> 
<p>If you see this page, the nginx web server is successfully installed and 
working. Further configuration is required.</p> 
 
<p>For online documentation and support please refer to 
<a href="http://nginx.org/">nginx.org</a>.<br/> 
Commercial support is available at 
<a href="http://nginx.com/">nginx.com</a>.</p> 
 
<p><em>Thank you for using nginx.</em></p> 
</body> 
</html> 
$  

--------------------
22Aug24:
Limits/Resources:
    
    Create deployment and assign limits and requests:    

    [root@master ~]# cat resdeploy.yml

    apiVersion: apps/v1

    kind: Deployment

    metadata:

      creationTimestamp: null

      labels:

        app: resdeploy

      name: resdeploy

    spec:

      replicas: 1

      selector:

        matchLabels:

          app: resdeploy

      strategy: {}

      template:

        metadata:

          creationTimestamp: null

          labels:

            app: resdeploy

        spec:

          containers:

          - image: nginx

            name: nginx

            resources: 

              limits:

                cpu: 200m

                memory: 200Mi

              requests:

                cpu: 100m

                memory: 100Mi

    status: {}

    [root@master ~]# 


    [root@master ~]# kubectl get pod -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE                NOMINATED NODE   READINESS GATES
resdeploy-79b575988b-vtvc4   1/1     Running   0          18m   192.168.0.4   node2.example.com   <none>           <none>
[root@master ~]# 

Change the values in requests/limits:

[root@master ~]# cat resdeploy.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: resdeploy
  name: resdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resdeploy
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: resdeploy
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: 
          limits:
            cpu: 200m
            memory: 200Gi
          requests:
            cpu: 100m
            memory: 100Gi
status: {}
[root@master ~]# 

[root@master ~]# kubectl get pod 
NAME                         READY   STATUS    RESTARTS   AGE
resdeploy-86bcb759c5-xpjfc   0/1     Pending   0          2s
[root@master ~]# 

[root@master ~]# kubectl get events | grep -i insuffi
42s         Warning   FailedScheduling         pod/resdeploy-86bcb759c5-xpjfc    0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient memory. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
[root@master ~]# 

Now change back to original values and find the status:

    [root@master ~]# cat resdeploy.yml

    apiVersion: apps/v1

    kind: Deployment

    metadata:

      creationTimestamp: null

      labels:

        app: resdeploy

      name: resdeploy

    spec:

      replicas: 1

      selector:

        matchLabels:

          app: resdeploy

      strategy: {}

      template:

        metadata:

          creationTimestamp: null

          labels:

            app: resdeploy

        spec:

          containers:

          - image: nginx

            name: nginx

            resources: 

              limits:

                cpu: 200m

                memory: 200Mi

              requests:

                cpu: 100m

                memory: 100Mi

    status: {}

    [root@master ~]# 


    [root@master ~]# kubectl get pod -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE                NOMINATED NODE   READINESS GATES
resdeploy-79b575988b-vtvc4   1/1     Running   0          18m   192.168.0.4   node2.example.com   <none>           <none>
[root@master ~]# 

[root@master ~]# kubectl describe node node2
Name:               node2.example.com
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node2.example.com
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"1a:86:a4:20:a0:f5"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.0.11.103
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 20 Aug 2024 22:46:14 +0530
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  node2.example.com
  AcquireTime:     <unset>
  RenewTime:       Thu, 22 Aug 2024 10:28:42 +0530
......
Addresses:
  InternalIP:  10.0.11.103
  Hostname:    node2.example.com
Capacity:
  cpu:                2
  ephemeral-storage:  64157076Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7882440Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  59127161144
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7780040Ki
  pods:               110
System Info:
  Machine ID:                 208eb4086c764575b018e1a1e227670d
  System UUID:                b2090842-45fa-d9d0-8017-c0b380dc5d49
  Boot ID:                    6ee665d7-f230-413b-9525-a13b2b777c9e
  Kernel Version:             4.18.0-553.el8_10.x86_64
  OS Image:                   Rocky Linux 8.10 (Green Obsidian)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.10
  Kubelet Version:            v1.29.8
  Kube-Proxy Version:         v1.29.8
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          (4 in total)
  Namespace                   Name                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                           ------------  ----------  ---------------  -------------  ---
  classwork                   resdeploy-79b575988b-9hmwr     100m (5%)     200m (10%)  100Mi (1%)       200Mi (2%)     14s
  kube-flannel                kube-flannel-ds-tglvz          100m (5%)     0 (0%)      50Mi (0%)        0 (0%)         35h
  kube-system                 fluentd-elasticsearch-xrmz9    100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)     35h
  kube-system                 kube-proxy-28bgv               0 (0%)        0 (0%)      0 (0%)           0 (0%)         35h
==============
$ kubectl create deploy resdeploy  - -image=nginx --dry-run=client -o yaml > resdeploy.yml
==============

DaemonSet:
    Create a DaemonSet with the below file.  
    Note: Using deployment dry-run you can create with modifications
[root@master ~]# cat daemonsetUP.yml
apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: mydset
  labels:
    app: dset-ds
    type: mydset
spec:
  template:
    metadata:
      name: mydset-pod
      labels:
        app: dset-pod
        type: mydset
    spec:
      containers:
      - image: nginx
        name: mydsetcont
  selector:
    matchLabels:
      type: mydset

[root@master ~]# 
[root@master ~]# kubectl create -f daemonsetUP.yml 

[root@master ~]# kubectl get ds
NAME     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
mydset   2         2         2       2            2           <none>          33m
[root@master ~]# 

Below output shows that 2 different instances are running in 2 different nodes (each node one instance)
[root@master ~]# kubectl get pod -o wide
NAME           READY   STATUS    RESTARTS   AGE     IP             NODE                NOMINATED NODE   READINESS GATES
mydset-gxvw4   1/1     Running   0          34m     192.168.0.6    node2.example.com   <none>           <none>
mydset-zfz6w   1/1     Running   0          29m     192.168.0.14   node1.example.com   <none>           <none>
[root@master ~]# 

========Excercise completed===================

Create a new node (node3.example.com) and include the same in this cluster with join command:
    Once included, then we can able to see the Daemonset is running on the new node also.  
    
[root@master ~]# kubectl get ds
NAME     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
mydset   3         3         3       3            3           <none>          43m
[root@master ~]# kubectl get pod -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP             NODE                NOMINATED NODE   READINESS GATES
mydset-9p7wt   1/1     Running   0          17m   192.168.0.2    node3.example.com   <none>           <none>
mydset-gxvw4   1/1     Running   0          43m   192.168.0.6    node2.example.com   <none>           <none>
mydset-zfz6w   1/1     Running   0          38m   192.168.0.14   node1.example.com   <none>           <none>
[root@master ~]# 

Without any commands, by default the new node is running with the above pods which are considered as DaemonSet (Agents on nodes):
    ====

Storage:
    
 StorageClass:
 [root@master murali-21aug]# vim storageclass.yml
apiVersion: storage.k8s.io/v1 
kind: StorageClass 
metadata: 
  name: localdisk 
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true

[root@master murali-21aug]# 

[root@master murali-21aug]#  kubectl create -f storageclass.yml
[root@master murali-21aug]#   kubectl get sc

Note: this StorageClass is a global object

PersistentVolume:
    
[root@master murali-21aug]# vim host-pv.yml 
kind: PersistentVolume 
apiVersion: v1 
metadata: 
   name: host-pv 
spec: 
   storageClassName: localdisk
   persistentVolumeReclaimPolicy: Recycle 
   capacity: 
      storage: 1Gi 
   accessModes: 
      - ReadWriteOnce 
   hostPath: 
      path: /var/output          ---------------------> host mountpoint 
------save----- 

[root@master murali-21aug]# kubectl create -f host-pv.yml
[root@master murali-21aug]# kubectl get pv
Note:  PV is also a global object - which we can able to access from any namespace (project)

PersistentVolumeClaim(PVC):
    
    [root@master murali-21aug]# vim host-pvc.yml
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
   name: host-pvc 
spec: 
   storageClassName: localdisk 
   accessModes: 
      - ReadWriteOnce 
   resources: 
      requests: 
         storage: 500Mi
[root@master murali-21aug]# 

Note:  PVC is a namespace specific
    
App/Pod Creation using Deployment:

[root@master murali-21aug]# vim deployment-hostpv.yml 
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: busybox
        name: busycont
        command: ['sh', '-c', 'while true; do echo Success! >> /output/success.txt; sleep 5; done'] 
        volumeMounts:
        - name: pv-storage
          mountPath: /output   --> Mountpoint on the pod 
      volumes:
      - name: pv-storage
        persistentVolumeClaim:
          claimName: host-pvc
  replicas: 1
  selector:
    matchLabels:
      type: front-end

[root@master murali-21aug]# 

Note: Pod is always a namespace specific

=======

[root@master 22Aug24]#vim nodename.yml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 8080
  nodeName: node3.example.com
[root@master 22Aug24]# 

[root@master 22Aug24]# kubectl create -f nodename.yml 
--> check the pod is schedule on the node3.example.com (in my case)

Cleanup above steps

NodeSelector:

[root@master 22Aug24]# cat nodeselector.yml 
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: nginx
  nodeSelector:
    env: prod
[root@master 22Aug24]# 

[root@master 22Aug24]# kubectl label node node1.example.com env=prod

[root@master 22Aug24]# kubectl create -f nodeselector.yml
---> check that the pod is running on node1 as its labled with env=prod

Taint/Tolerations:

#kubectl taint node node1.example.com color=orange:NoSchedule 
#kubectl describe node node1.example.com | grep Taint
-- verify the node is tainted----
#kubectl create -f deployment.yml 
-- note that the deployment will not go to node1 as its tainted. 

NoSchedule:  No further deployments will be allowed but not bother about existing pods in the node
NoExecute: No further deployments will be allowed but will evict the existing pods also in the node
PreferrableSchedule:  Worst case it will allow to deploy the pod.

Check with multiple options --- check for recordings 











=====
Helm Charts:
    
    https://helm.sh/docs/intro/install/
    
      $helm search repo bitnami
  $helm repo add bitnami https://charts.bitnami.com/bitnami


     $ helm repo update              

     

     $ helm install bitnami/mysql --generate-name


       $ helm list

 ======
