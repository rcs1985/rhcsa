kubPre-requisite:
    
#yum remove -y podman-docker  

K8S Cluster Install:
    
Kubernetes Cluster Setup Installation - 1Master, 2WorkerNodes:
 
 Note:  All the below commands should be executed in all 3 Nodes:   

# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6q

10.0.7.1    master master          (take IP from "ip a" command and place it here)
10.0.7.2    node1 worker1          (take IP from "ip a" command and place it here)
10.0.7.3    node2 worker2          (take IP from "ip a" command and place it here)

#swapoff -a (to permanently disable swap, make entry on /etc/fstab)

#systemctl stop firewalld ; systemctl disable firewalld; systemctl status firewalld

#setenforce 0

#yum install wget -y 

#wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.6.10-3.1.el7.x86_64.rpm

#yum localinstall containerd.io-1.6.10-3.1.el7.x86_64.rpm -y

#systemctl start  containerd.service; systemctl enable containerd.service; systemctl status containerd.service

#cat<<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# modprobe overlay; sudo modprobe br_netfilter



#cat<<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

#sudo sysctl --system
      
cat<<EOF| sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
      
      
      
      


#sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config


#yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

#systemctl enable --now kubelet

Note:  The below command for Pre-Flight - It will show us if the previous commands configured properly:  This should also be executed 
            in all 3 nodes

#kubeadm init phase preflight

Note: IF there is any error in above command, follow below steps

[root@master ~]# kubeadm init phase preflight
I0621 07:18:50.195909    5747 version.go:256] remote version is much newer: v1.30.2; falling back to: stable-1.29
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2024-06-21T07:18:50Z" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

Note:  The above is known error:  To fix, please follow the below commands:

#rm /etc/containerd/config.toml

#systemctl restart containerd.service

Now preflight ...

#kubeadm init phase preflight


-=======================================================================================================
Should be run only in Master Node:
========================================================================================================

master#kubeadm init --pod-network-cidr=192.168.0.0/24    --> Please note that the output of this command should be saved in a file on master node:

.....

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:192.168.0.60  master1.example.com  master1
192.168.0.59  master1.example.com  master2
192.168.0.61  node2.example.com node2
192.168.0.62  node3.example.com node3


  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.128.0.10:6443 --token x5x7o7.25dctlxi91sg6jqn --discovery-token-ca-cert-hash sha256:eb69c97473f2f7b0de6286230449b8f94ed92dd9d55c02e3c7453e1

=============

master#export KUBECONFIG=/etc/kubernetes/admin.conf


[root@master ~]# kubectl get node
NAME                   STATUS     ROLES           AGE    VERSION
master   NotReady   control-plane   2m5s   v1.25.4
[root@master ~]#

[root@master ~]# kubectl get pod -A
NAMESPACE     NAME                                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-565d847f94-9ffg6                       0/1     Pending   0          4m33s
kube-system   coredns-565d847f94-hfk2t                       0/1     Pending   0          4m33s



=======


wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

=======

[root@master ~]# vim kube-flannel.yml      -->> Change this file with below subnet (which is already we have used while init command)
      "Network": "192.168.0.0/24",       ====> change here for our requried pod network
      "Backend": {
        "Type": "vxlan"
      }
    }
========

[root@master ~]# kubectl create -f kube-flannel.yml
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
[root@master ~]#



[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0          30s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0          16m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0          16m

===

Now Master is ready:
[root@master ~]# kubectl get node
NAME                   STATUS   ROLES           AGE   VERSION
master  Ready    control-plane   17m   v1.25.4
[root@master ~]#


Joining worker node1: 
===============
In Worker1 node:  -> "take the below command from your earlier kubeadm init ... output"
===============
[root@node1 ~]# kubeadm join 10.0.7.1:6443 --token xf3lrl.fb7cniz8139fhvwx --discovery-token-ca-cert-hash sha256:1abb0f47f36b68ba0ce1e2373ab30515f87e936e2f334feb643aeee1ed0529f2

Error in Flannel:

[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS      AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0             3m43s
kube-flannel   kube-flannel-ds-rtfwv                          0/1     Error     3 (44s ago)   88s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0             19m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0             19m

Note: To fix the error, use below patch command:

Note: The below command should be done only on Master node:

[root@master ~]# kubectl patch node worker1 -p '{"spec":{"podCIDR":"192.168.0.0/24"}}'
node/node1 patched
[root@master ~]#


[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS        AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0               8m44s
kube-flannel   kube-flannel-ds-rtfwv                          1/1     Running   6 (2m56s ago)   6m29s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0               24m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0               24m



Now joining worker2 node:
 

[root@node2 mnt]# rm /etc/containerd/config.toml
rm: remove regular file '/etc/containerd/config.toml'? y
[root@node2 mnt]# systemctl restart containerd.service
[root@node2 mnt]#


[root@node2 mnt]# kubeadm join 10.0.7.1:6443 --token xf3lrl.fb7cniz8139fhvwx --discovery-token-ca-cert-hash sha256:1abb0f47f36b68ba0ce1e2373ab30515f87e936e2f334feb643aeee1ed0529f2


[root@master ~]# kubectl get node
NAME                    STATUS   ROLES           AGE   VERSION
master    Ready    control-plane   32m   v1.25.4
node1   Ready    <none>          13m   v1.25.4
node2   Ready    <none>          65s   v1.25.4
[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS      AGE
kube-flannel   kube-flannel-ds-66rzn                          0/1     Error     3 (37s ago)   91s
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0             16m


This needs to be patched to fix flannel error:

[root@master ~]# kubectl patch node node2 -p '{"spec":{"podCIDR":"192.168.0.0/24"}}'
node/node2 patched
[root@master ~]#

----------------------------------------------------
if the server rebooted, swap settings will get revert (if not disabled permanently). nodes wont get start automatically if swap enabled

Perform below steps to start the nodes

On all the nodes
------------------------------
swapoff -a
systemctl stop kubelet
stystemctl start kubelet

export KUBECONFIG=/etc/kubernetes/admin.conf (only on master node)

kebectl get nodes (on all the nodes, this should start the nodes)

20Aug24:

Yaml Basics:
    
1. Dictionary words
Name: Murali
Location: Chennai
2. List
Fruits:
    - Apple
    - Orange
    - Banana
    
Address:
    -  No.10, Ist MainRoad,
       Shashtri Nagar
       Adyar, Tvm Road, Chennai
    - 12, Ist Cross Street,
       Post Office Street,
       K.K.Nagar, Chennai
    -  10, 2nd street,
       Postal and Telegraph Nagar
       Madurai

3. Parent/Child Relationship:
    
    #useradd user300
    user:
    name: rjp
    state: present
-- its wrong---
 
               user:
                    name: rjpuser
                    state: present
                    
    Pod Creation:
        
    [root@master ~]# kubectl run myweb2 --image=httpd --dry-run=client -o yaml > pod.yml
    [root@master ~]# cat pod.yml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: myweb2
  name: myweb2
spec:
  containers:
  - image: httpd
    name: myweb2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
[root@master ~]# 

[root@master ~]# kubectl create -f pod.yml
[root@master ~]# kubectl get pod 
[root@master ~]#  kubectl get pod -o wide 
[root@master ~]#  kubectl describe pod myweb2

----
ReplicaSet:
    
[root@master ~]# vm replicaset.yml 
apiVersion: apps/v1i
kind: ReplicaSet
metadata: 
  name: myapps-rs
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - image: nginx
        name: mypod
  replicas: 4
  selector:
    matchLabels:
      type: front-end
----save---

$kubectl create -f replicaset.yml
$kubectl get pods -o wide
edit the "replicas: 4" on above yaml file and try apply the same
$kubectl apply -f replicaset.yml 
$kubectl get pod -o wide

---
Deployment:
    
    Ensure that no resources are available in your namespace 
    [root@master ~]# kubectl get pod 
    --------nothing should be running here------
[root@master ~]# vim deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: nginx
        name: mycont
  replicas: 4 deI
  selector:
    matchLabels:
      type: front-end

-------
[root@master ~]#kubectl create -f deployment.yml
[root@master ~]#kubectl get pod -o wide -w 

Apply a change now on the file:

[root@master ~]# vim deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: httpd  --> applied
        name: mycont
  replicas: 4
  selector:
    matchLabels:
      type: front-end
---
[root@master ~]#kubectl apply -f deployment.yml
[root@master ~]#kubectl get pod -o wide -w
---Able to see the deployment happenning based on the stratergy -----
Do similar changes/increase replica count and dirt your hands.
-------------------

