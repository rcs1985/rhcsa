kubekubPre-requisite:
    
#yum remove -y podman-docker  

K8S Cluster Install:
    
Kubernetes Cluster Setup Installation - 1Master, 2WorkerNodes:
 
 Note:  All the below commands should be executed in all 3 Nodes:   

# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6q

10.0.7.1    master master          (take IP from "ip a" command and place it here)
10.0.7.2    node1 worker1          (take IP from "ip a" command and place it here)
10.0.7.3    node2 worker2          (take IP from "ip a" command and place it here)

#swapoff -a (to permanently disable swap, make entry on /etc/fstab)

#systemctl stop firewalld ; systemctl disable firewalld; systemctl status firewalld

#setenforce 0

#yum install wget -y 

#wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.6.10-3.1.el7.x86_64.rpm

#yum localinstall containerd.io-1.6.10-3.1.el7.x86_64.rpm -y

#systemctl start  containerd.service; systemctl enable containerd.service; systemctl status containerd.service

#cat<<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# modprobe overlay; sudo modprobe br_netfilter



#cat<<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

#sudo sysctl --system
      
cat<<EOF| sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
      
      
      
      


#sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config


#yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

#systemctl enable --now kubelet

Note:  The below command for Pre-Flight - It will show us if the previous commands configured properly:  This should also be executed 
            in all 3 nodes

#kubeadm init phase preflight

Note: IF there is any error in above command, follow below steps

[root@master ~]# kubeadm init phase preflight
I0621 07:18:50.195909    5747 version.go:256] remote version is much newer: v1.30.2; falling back to: stable-1.29
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2024-06-21T07:18:50Z" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

Note:  The above is known error:  To fix, please follow the below commands:

#rm /etc/containerd/config.toml

#systemctl restart containerd.service

Now preflight ...

#kubeadm init phase preflight


-=======================================================================================================
Should be run only in Master Node:
========================================================================================================

master#kubeadm init --pod-network-cidr=192.168.0.0/24    --> Please note that the output of this command should be saved in a file on master node:

.....

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:192.168.0.60  master1.example.com  master1
192.168.0.59  master1.example.com  master2
192.168.0.61  node2.example.com node2
192.168.0.62  node3.example.com node3


  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.128.0.10:6443 --token x5x7o7.25dctlxi91sg6jqn --discovery-token-ca-cert-hash sha256:eb69c97473f2f7b0de6286230449b8f94ed92dd9d55c02e3c7453e1

=============

master#export KUBECONFIG=/etc/kubernetes/admin.conf


[root@master ~]# kubectl get node
NAME                   STATUS     ROLES           AGE    VERSION
master   NotReady   control-plane   2m5s   v1.25.4
[root@master ~]#

[root@master ~]# kubectl get pod -A
NAMESPACE     NAME                                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-565d847f94-9ffg6                       0/1     Pending   0          4m33s
kube-system   coredns-565d847f94-hfk2t                       0/1     Pending   0          4m33s



=======


wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

=======

[root@master ~]# vim kube-flannel.yml      -->> Change this file with below subnet (which is already we have used while init command)
      "Network": "192.168.0.0/24",       ====> change here for our requried pod network
      "Backend": {
        "Type": "vxlan"
      }
    }
========

[root@master ~]# kubectl create -f kube-flannel.yml
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
[root@master ~]#



[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0          30s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0          16m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0          16m

===

Now Master is ready:
[root@master ~]# kubectl get node
NAME                   STATUS   ROLES           AGE   VERSION
master  Ready    control-plane   17m   v1.25.4
[root@master ~]#


Joining worker node1: 
===============
In Worker1 node:  -> "take the below command from your earlier kubeadm init ... output"
===============
[root@node1 ~]# kubeadm join 10.0.7.1:6443 --token xf3lrl.fb7cniz8139fhvwx --discovery-token-ca-cert-hash sha256:1abb0f47f36b68ba0ce1e2373ab30515f87e936e2f334feb643aeee1ed0529f2

Error in Flannel:

[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS      AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0             3m43s
kube-flannel   kube-flannel-ds-rtfwv                          0/1     Error     3 (44s ago)   88s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0             19m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0             19m

Note: To fix the error, use below patch command:

Note: The below command should be done only on Master node:

[root@master ~]# kubectl patch node worker1 -p '{"spec":{"podCIDR":"192.168.0.0/24"}}'
node/node1 patched
[root@master ~]#


[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS        AGE
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0               8m44s
kube-flannel   kube-flannel-ds-rtfwv                          1/1     Running   6 (2m56s ago)   6m29s
kube-system    coredns-565d847f94-9ffg6                       1/1     Running   0               24m
kube-system    coredns-565d847f94-hfk2t                       1/1     Running   0               24m




Now joining worker2 node:
 

[root@node2 mnt]# rm /etc/containerd/config.toml
rm: remove regular file '/etc/containerd/config.toml'? y
[root@node2 mnt]# systemctl restart containerd.service
[root@node2 mnt]#


[root@node2 mnt]# kubeadm join 10.0.7.1:6443 --token xf3lrl.fb7cniz8139fhvwx --discovery-token-ca-cert-hash sha256:1abb0f47f36b68ba0ce1e2373ab30515f87e936e2f334feb643aeee1ed0529f2


[root@master ~]# kubectl get node
NAME                    STATUS   ROLES           AGE   VERSION
master    Ready    control-plane   32m   v1.25.4
node1   Ready    <none>          13m   v1.25.4
node2   Ready    <none>          65s   v1.25.4
[root@master ~]# kubectl get pod -A
NAMESPACE      NAME                                           READY   STATUS    RESTARTS      AGE
kube-flannel   kube-flannel-ds-66rzn                          0/1     Error     3 (37s ago)   91s
kube-flannel   kube-flannel-ds-plr6g                          1/1     Running   0             16m


This needs to be patched to fix flannel error:

[root@master ~]# kubectl patch node node2 -p '{"spec":{"podCIDR":"192.168.0.0/24"}}'
node/node2 patched
[root@master ~]#

Any issues on joining node - use below command to recreate:
#kubeadm token create --print-join-command

----------------------------------------------------
if the server rebooted, swap settings will get revert (if not disabled permanently). nodes wont get start automatically if swap enabled

Perform below steps to start the nodes

On all the nodes
------------------------------
swapoff -a
systemctl stop kubelet
stystemctl start kubelet

export KUBECONFIG=/etc/kubernetes/admin.conf (only on master node)

kebectl get nodes (on all the nodes, this should start the nodes)

20Aug24:

Yaml Basics:
    
1. Dictionary words
Name: Murali
Location: Chennai
2. List
Fruits:
    - Apple
    - Orange
    - Banana
    
Address:
    -  No.10, Ist MainRoad,
       Shashtri Nagar
       Adyar, Tvm Road, Chennai
    - 12, Ist Cross Street,
       Post Office Street,
       K.K.Nagar, Chennai
    -  10, 2nd street,
       Postal and Telegraph Nagar
       Madurai

3. Parent/Child Relationship:
    
    #useradd user300

        user:

        name: rjp

        state: present

    -- its wrong---

     

               user:
                    name: rjpuser
                    state: present
                    
    Pod Creation:
        
    [root@master ~]# kubectl run myweb2 --image=httpd --dry-run=client -o yaml > pod.yml
    [root@master ~]# cat pod.yml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: myweb2
  name: myweb2
spec:
  containers:
  - image: httpd
    name: myweb2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
[root@master ~]# 

[root@master ~]# kubectl create -f pod.yml
[root@master ~]# kubectl get pod 
[root@master ~]#  kubectl get pod -o wide 
[root@master ~]#  kubectl describe pod myweb2

----
ReplicaSet:
    
[root@master ~]# vm replicaset.yml 
apiVersion: apps/v1
kind: ReplicaSet
metadata: 
  name: myapps-rs
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - image: nginx
        name: mypod
  replicas: 4
  selector:
    matchLabels:
      type: front-end
----save---

$kubectl create -f replicaset.yml
$kubectl get pods -o wide
edit the "replicas: 4" on above yaml file and try apply the same
$kubectl apply -f replicaset.yml 
$kubectl get pod -o wide

---
Deployment:
    
    Ensure that no resources are available in your namespace 
    [root@master ~]# kubectl get pod 
    --------nothing should be running here------
[root@master ~]# vim deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: nginx
        name: mycont
  replicas: 4 deI
  selector:
    matchLabels:
      type: front-end

-------
[root@master ~]#kubectl create -f deployment.yml
[root@master ~]#kubectl get pod -o wide -w 

Apply a change now on the file:

[root@master ~]# vim deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: httpd  --> applied
        name: mycont
  replicas: 4
  selector:
    matchLabels:
      type: front-end
---
[root@master ~]#kubectl apply -f deployment.yml
[root@master ~]#kubectl get pod -o wide -w
---Able to see the deployment happenning based on the stratergy -----
Do similar changes/increase replica count and dirt your hands.
-------------------
21Aug24:
NameSpace: (project in openshift)
    
    #kubectl create ns mytcs --dry-run=client -o yaml > ns.yml
    #cat ns.yml    ---> just verify the yaml file
    #kubectl create -f ns.yml
    #kubectl get ns  --> will give you all other avlbl namespaces 
    #kubectl get ns mytcs -o yaml
    #kubectl get all
    #kubectl get pod -n default  -> to check the resources in other namespaces 
    How to change namespace?
    #kubectl config set-context $(kubectl config current-context) --namespace=mytcs     
    
    Go to the other namespace and try create resources:
    #kubectl config set-context $(kubectl config current-context) --namespace=default
    #kubectl create -f replicaset.yml
    #kubectl  get pod 
    #kubectl get pod -n mytcs 
    
    To check the resources in all other namespaces:
    #kubectl  get pod -A
    
    To check the current Namespace we are in:
        kubectl config get-contexts
    
    Service:
    

        $ kubectl create deploy nginxsvc  - -image=nginx --dry-run=client -o yaml > nginxsvc.yml

    $ kubectl create â€“f nginxsvc.yml 

 List all the objects using 'kubectl get all' 
    $ kubectl get all 

 Expose the service
    $ kubectl expose deploy nginxsvc --port=80  
    $kubectl describe svc nginxsvc  
Name:              nginxsvc 
Namespace:         default 
Labels:            app=nginxsvc 
Annotations:       <none> 
Selector:          app=nginxsvc 
Type:              ClusterIP 
IP Family Policy:  SingleStack 
IP Families:       IPv4 
IP:                10.96.24.47 
IPs:               10.96.24.47 
Port:              <unset>  80/TCP 
TargetPort:        80/TCP 
Endpoints:         10.244.1.3:80,10.244.1.4:80,10.244.2.3:80 
Session Affinity:  None 
Events:            <none> 
$  
 $ kubectl get all 

 Get the yaml output of the service. 
 
$ kubectl get svc nginxsvc -o yaml  
apiVersion: v1 
kind: Service 
metadata: 
  creationTimestamp: "2023-11-16T09:33:08Z" 
  labels: 
    app: nginxsvc 
  name: nginxsvc 
  namespace: default 
  resourceVersion: "39621" 
  uid: a76667f4-b058-44d4-bd02-796ad6d23278 
spec: 
  clusterIP: 10.96.24.47 
  clusterIPs: 
  - 10.96.24.47 
  internalTrafficPolicy: Cluster 
  ipFamilies: 
  - IPv4 
  ipFamilyPolicy: SingleStack 
  ports: 
  - port: 80 
    protocol: TCP 
    targetPort: 80 
  selector: 
    app: nginxsvc 
  sessionAffinity: None 
  type: ClusterIP 
status: 
  loadBalancer: {} 
$  

 Gather the endpoint and service IP. 
$ kubectl get endpoints  
NAME         ENDPOINTS                                   AGE 
kubernetes   172.18.0.4:6443                             14h 
nginxsvc     10.244.1.3:80,10.244.1.4:80,10.244.2.3:80   2m41s 
$  
$ kubectl get svc  
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE 
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   14h 
nginxsvc     ClusterIP   10.96.24.47   <none>        80/TCP    2m51s 
$  

Edit the service and update the type as NodePort. After editing the contents should be similar to next step
$ kubectl edit svc nginxsvc  
service/nginxsvc edited 

Get the yaml form of service configuration. Additions highlighted in bold. 
$ kubectl get svc nginxsvc -o yaml  
apiVersion: v1 
kind: Service 
metadata: 
  creationTimestamp: "2023-11-16T09:33:08Z" 
  labels: 
    app: nginxsvc 
  name: nginxsvc 
  namespace: default 
  resourceVersion: "40350" 
  uid: a76667f4-b058-44d4-bd02-796ad6d23278 
spec: 
  clusterIP: 10.96.24.47 
  clusterIPs: 
  - 10.96.24.47 
  externalTrafficPolicy: Cluster 
  internalTrafficPolicy: Cluster 
  ipFamilies: 
  - IPv4 
  ipFamilyPolicy: SingleStack 
  ports: 
  - **nodePort: 32000** 
    port: 80 
    protocol: TCP 
    targetPort: 80 
  selector: 
    app: nginxsvc 
  sessionAffinity: None 
  **type: NodePort** 
status: 
  loadBalancer: {} 

 Find the node in which application pod is running. 
$ kubectl get pod -o wide  
NAME                        READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES 
nginxsvc-6f45cc47b4-2qcbb   1/1     Running   0          8m51s   10.244.2.3   kind-worker2   <none>           <none> 
nginxsvc-6f45cc47b4-6cw7z   1/1     Running   0          9m18s   10.244.1.3   kind-worker    <none>           <none> 
nginxsvc-6f45cc47b4-l7x4k   1/1     Running   0          8m51s   10.244.1.4   kind-worker    <none>           <none> 
$  

Access the application using node IP and nodePort. 
$ curl http://172.18.0.3:32000 
<!DOCTYPE html> 
<html> 
<head> 
<title>Welcome to nginx!</title> 
<style> 
html { color-scheme: light dark; } 
body { width: 35em; margin: 0 auto; 
font-family: Tahoma, Verdana, Arial, sans-serif; } 
</style> 
</head> 
<body> 
<h1>Welcome to nginx!</h1> 
<p>If you see this page, the nginx web server is successfully installed and 
working. Further configuration is required.</p> 
 
<p>For online documentation and support please refer to 
<a href="http://nginx.org/">nginx.org</a>.<br/> 
Commercial support is available at 
<a href="http://nginx.com/">nginx.com</a>.</p> 
 
<p><em>Thank you for using nginx.</em></p> 
</body> 
</html> 
$  

--------------------
22Aug24:
Limits/Resources:
    
    Create deployment and assign limits and requests:    

    [root@master ~]# cat resdeploy.yml

    apiVersion: apps/v1

    kind: Deployment

    metadata:

      creationTimestamp: null

      labels:

        app: resdeploy

      name: resdeploy

    spec:

      replicas: 1

      selector:

        matchLabels:

          app: resdeploy

      strategy: {}

      template:

        metadata:

          creationTimestamp: null

          labels:

            app: resdeploy

        spec:

          containers:

          - image: nginx

            name: nginx

            resources: 

              limits:

                cpu: 200m

                memory: 200Mi

              requests:

                cpu: 100m

                memory: 100Mi

    status: {}

    [root@master ~]# 


    [root@master ~]# kubectl get pod -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE                NOMINATED NODE   READINESS GATES
resdeploy-79b575988b-vtvc4   1/1     Running   0          18m   192.168.0.4   node2.example.com   <none>           <none>
[root@master ~]# 

Change the values in requests/limits:

[root@master ~]# cat resdeploy.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: resdeploy
  name: resdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resdeploy
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: resdeploy
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: 
          limits:
            cpu: 200m
            memory: 200Gi
          requests:
            cpu: 100m
            memory: 100Gi
status: {}
[root@master ~]# 

[root@master ~]# kubectl get pod 
NAME                         READY   STATUS    RESTARTS   AGE
resdeploy-86bcb759c5-xpjfc   0/1     Pending   0          2s
[root@master ~]# 

[root@master ~]# kubectl get events | grep -i insuffi
42s         Warning   FailedScheduling         pod/resdeploy-86bcb759c5-xpjfc    0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient memory. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
[root@master ~]# 

Now change back to original values and find the status:

    [root@master ~]# cat resdeploy.yml

    apiVersion: apps/v1

    kind: Deployment

    metadata:

      creationTimestamp: null

      labels:

        app: resdeploy

      name: resdeploy

    spec:

      replicas: 1

      selector:

        matchLabels:

          app: resdeploy

      strategy: {}

      template:

        metadata:

          creationTimestamp: null

          labels:

            app: resdeploy

        spec:

          containers:

          - image: nginx

            name: nginx

            resources: 

              limits:

                cpu: 200m

                memory: 200Mi

              requests:

                cpu: 100m

                memory: 100Mi

    status: {}

    [root@master ~]# 


    [root@master ~]# kubectl get pod -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE                NOMINATED NODE   READINESS GATES
resdeploy-79b575988b-vtvc4   1/1     Running   0          18m   192.168.0.4   node2.example.com   <none>           <none>
[root@master ~]# 

[root@master ~]# kubectl describe node node2
Name:               node2.example.com
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node2.example.com
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"1a:86:a4:20:a0:f5"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.0.11.103
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 20 Aug 2024 22:46:14 +0530
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  node2.example.com
  AcquireTime:     <unset>
  RenewTime:       Thu, 22 Aug 2024 10:28:42 +0530
......
Addresses:
  InternalIP:  10.0.11.103
  Hostname:    node2.example.com
Capacity:
  cpu:                2
  ephemeral-storage:  64157076Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7882440Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  59127161144
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7780040Ki
  pods:               110
System Info:
  Machine ID:                 208eb4086c764575b018e1a1e227670d
  System UUID:                b2090842-45fa-d9d0-8017-c0b380dc5d49
  Boot ID:                    6ee665d7-f230-413b-9525-a13b2b777c9e
  Kernel Version:             4.18.0-553.el8_10.x86_64
  OS Image:                   Rocky Linux 8.10 (Green Obsidian)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.10
  Kubelet Version:            v1.29.8
  Kube-Proxy Version:         v1.29.8
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          (4 in total)
  Namespace                   Name                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                           ------------  ----------  ---------------  -------------  ---
  classwork                   resdeploy-79b575988b-9hmwr     100m (5%)     200m (10%)  100Mi (1%)       200Mi (2%)     14s
  kube-flannel                kube-flannel-ds-tglvz          100m (5%)     0 (0%)      50Mi (0%)        0 (0%)         35h
  kube-system                 fluentd-elasticsearch-xrmz9    100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)     35h
  kube-system                 kube-proxy-28bgv               0 (0%)        0 (0%)      0 (0%)           0 (0%)         35h
==============
$ kubectl create deploy resdeploy  - -image=nginx --dry-run=client -o yaml > resdeploy.yml
==============

DaemonSet:
    Create a DaemonSet with the below file.  
    Note: Using deployment dry-run you can create with modifications
[root@master ~]# cat daemonsetUP.yml
apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: mydset
  labels:
    app: dset-ds
    type: mydset
spec:
  template:
    metadata:
      name: mydset-pod
      labels:
        app: dset-pod
        type: mydset
    spec:
      containers:
      - image: nginx
        name: mydsetcont
  selector:
    matchLabels:
      type: mydset

[root@master ~]# 
[root@master ~]# kubectl create -f daemonsetUP.yml 

[root@master ~]# kubectl get ds
NAME     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
mydset   2         2         2       2            2           <none>          33m
[root@master ~]# 

Below output shows that 2 different instances are running in 2 different nodes (each node one instance)
[root@master ~]# kubectl get pod -o wide
NAME           READY   STATUS    RESTARTS   AGE     IP             NODE                NOMINATED NODE   READINESS GATES
mydset-gxvw4   1/1     Running   0          34m     192.168.0.6    node2.example.com   <none>           <none>
mydset-zfz6w   1/1     Running   0          29m     192.168.0.14   node1.example.com   <none>           <none>
[root@master ~]# 

========Excercise completed===================

Create a new node (node3.example.com) and include the same in this cluster with join command:
    Once included, then we can able to see the Daemonset is running on the new node also.  
    
[root@master ~]# kubectl get ds
NAME     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
mydset   3         3         3       3            3           <none>          43m
[root@master ~]# kubectl get pod -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP             NODE                NOMINATED NODE   READINESS GATES
mydset-9p7wt   1/1     Running   0          17m   192.168.0.2    node3.example.com   <none>           <none>
mydset-gxvw4   1/1     Running   0          43m   192.168.0.6    node2.example.com   <none>           <none>
mydset-zfz6w   1/1     Running   0          38m   192.168.0.14   node1.example.com   <none>           <none>
[root@master ~]# 

Without any commands, by default the new node is running with the above pods which are considered as DaemonSet (Agents on nodes):
    ====

Storage:
    
 StorageClass:
 [root@master murali-21aug]# vim storageclass.yml
apiVersion: storage.k8s.io/v1 
kind: StorageClass 
metadata: 
  name: localdisk 
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true

[root@master murali-21aug]# 

[root@master murali-21aug]#  kubectl create -f storageclass.yml
[root@master murali-21aug]#   kubectl get sc

Note: this StorageClass is a global object

PersistentVolume:
    
[root@master murali-21aug]# vim host-pv.yml 
kind: PersistentVolume 
apiVersion: v1 
metadata: 
   name: host-pv 
spec: 
   storageClassName: localdisk
   persistentVolumeReclaimPolicy: Recycle 
   capacity: 
      storage: 1Gi 
   accessModes: 
      - ReadWriteOnce 
   hostPath: 
      path: /var/output          ---------------------> host mountpoint 
------save----- 

[root@master murali-21aug]# kubectl create -f host-pv.yml
[root@master murali-21aug]# kubectl get pv
Note:  PV is also a global object - which we can able to access from any namespace (project)

PersistentVolumeClaim(PVC):
    
    [root@master murali-21aug]# vim host-pvc.yml
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
   name: host-pvc 
spec: 
   storageClassName: localdisk 
   accessModes: 
      - ReadWriteOnce 
   resources: 
      requests: 
         storage: 500Mi
[root@master murali-21aug]# 

Note:  PVC is a namespace specific
    
App/Pod Creation using Deployment:

[root@master murali-21aug]# vim deployment-hostpv.yml 
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: myapps-dep
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-dep-pod
      labels:
        app: myapp-dep
        type: front-end
    spec:
      containers:
      - image: busybox
        name: busycont
        command: ['sh', '-c', 'while true; do echo Success! >> /output/success.txt; sleep 5; done'] 
        volumeMounts:
        - name: pv-storage
          mountPath: /output   --> Mountpoint on the pod 
      volumes:
      - name: pv-storage
        persistentVolumeClaim:
          claimName: host-pvc
  replicas: 1
  selector:
    matchLabels:
      type: front-end

[root@master murali-21aug]# 

Note: Pod is always a namespace specific

=======

[root@master 22Aug24]#vim nodename.yml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 8080
  nodeName: node3.example.com
[root@master 22Aug24]# 

[root@master 22Aug24]# kubectl create -f nodename.yml 
--> check the pod is schedule on the node3.example.com (in my case)

Cleanup above steps

NodeSelector:

[root@master 22Aug24]# cat nodeselector.yml 
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: nginx
  nodeSelector:
    env: prod
[root@master 22Aug24]# 

[root@master 22Aug24]# kubectl label node node1.example.com env=prod

[root@master 22Aug24]# kubectl create -f nodeselector.yml
---> check that the pod is running on node1 as its labled with env=prod

Taint/Tolerations:

#kubectl taint node node1.example.com color=orange:NoSchedule 
#kubectl describe node node1.example.com | grep Taint
-- verify the node is tainted----
#kubectl create -f deployment.yml 
-- note that the deployment will not go to node1 as its tainted. 

NoSchedule:  No further deployments will be allowed but not bother about existing pods in the node
NoExecute: No further deployments will be allowed but will evict the existing pods also in the node
PreferrableSchedule:  Worst case it will allow to deploy the pod.

Check with multiple options --- check for recordings 

==============


26Aug24:
    
    Node Maintenance - Cordon/Drain Activity:

[root@master ~]# kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE                NOMINATED NODE   READINESS GATES
resdeploy2-744758457d-79vjl   1/1     Running   0          16h   192.168.0.13   node1.example.com   <none>           <none>
resdeploy2-744758457d-nqd8r   1/1     Running   0          31s   192.168.0.3    node2.example.com   <none>           <none>
resdeploy2-744758457d-p22mz   1/1     Running   0          16h   192.168.0.4    node1.example.com   <none>           <none>
resdeploy2-744758457d-pxm26   1/1     Running   0          19s   192.168.0.4    node2.example.com   <none>           <none>
[root@master ~]# kubectl get nodes
NAME                 STATUS   ROLES           AGE     VERSION
master.example.com   Ready    control-plane   9d      v1.29.8
node1.example.com    Ready    <none>          9d      v1.29.8
node2.example.com    Ready    <none>          5d11h   v1.29.8
[root@master ~]# kubectl cordon node2.example.com
node/node2.example.com cordoned

[root@master ~]# kubectl get nodes
NAME                 STATUS                     ROLES           AGE     VERSION
master.example.com   Ready                      control-plane   9d      v1.29.8
node1.example.com    Ready                      <none>          9d      v1.29.8
node2.example.com    Ready,SchedulingDisabled   <none>          5d11h   v1.29.8
[root@master ~]# kubectl get pods -o wide
[root@master ~]# kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP             NODE                NOMINATED NODE   READINESS GATES
resdeploy2-744758457d-79vjl   1/1     Running   0          16h     192.168.0.13   node1.example.com   <none>           <none>
resdeploy2-744758457d-nqd8r   1/1     Running   0          8m16s   192.168.0.3    node2.example.com   <none>           <none>
resdeploy2-744758457d-p22mz   1/1     Running   0          16h     192.168.0.4    node1.example.com   <none>           <none>
resdeploy2-744758457d-pxm26   1/1     Running   0          8m4s    192.168.0.4    node2.example.com   <none>           <none>
[root@master ~]# kubectl scale --replicas=6 deploy resdeploy
error: no objects passed to scale
[root@master ~]# kubectl scale --replicas=6 deploy resdeploy2
deployment.apps/resdeploy2 scaled
[root@master ~]# kubectl get pods -o wide


[root@master ~]# kubectl get pods -o wide
NAME                          READY   STATUS              RESTARTS   AGE     IP             NODE                NOMINATED NODE   READINESS GATES
resdeploy2-744758457d-79vjl   1/1     Running             0          16h     192.168.0.13   node1.example.com   <none>           <none>
resdeploy2-744758457d-kzppd   0/1     ContainerCreating   0          4s      <none>         node1.example.com   <none>           <none>
resdeploy2-744758457d-nqd8r   1/1     Running             0          8m49s   192.168.0.3    node2.example.com   <none>           <none>
resdeploy2-744758457d-p22mz   1/1     Running             0          16h     192.168.0.4    node1.example.com   <none>           <none>
resdeploy2-744758457d-pxm26   1/1     Running             0          8m37s   192.168.0.4    node2.example.com   <none>           <none>
resdeploy2-744758457d-tgms5   0/1     ContainerCreating   0          3s      <none>         node1.example.com   <none>           <none>
[root@master ~]# kubectl get pods -o wide


[root@master ~]# kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP             NODE                NOMINATED NODE   READINESS GATES
resdeploy2-744758457d-79vjl   1/1     Running   0          16h     192.168.0.13   node1.example.com   <none>           <none>
resdeploy2-744758457d-kzppd   1/1     Running   0          17s     192.168.0.14   node1.example.com   <none>           <none>
resdeploy2-744758457d-nqd8r   1/1     Running   0          9m2s    192.168.0.3    node2.example.com   <none>           <none>
resdeploy2-744758457d-p22mz   1/1     Running   0          16h     192.168.0.4    node1.example.com   <none>           <none>
resdeploy2-744758457d-pxm26   1/1     Running   0          8m50s   192.168.0.4    node2.example.com   <none>           <none>
resdeploy2-744758457d-tgms5   1/1     Running   0          16s     192.168.0.15   node1.example.com   <none>           <none>
[root@master ~]# kubectl get nodes
NAME                 STATUS                     ROLES           AGE     VERSION
master.example.com   Ready                      control-plane   9d      v1.29.8
node1.example.com    Ready                      <none>          9d      v1.29.8
node2.example.com    Ready,SchedulingDisabled   <none>          5d11h   v1.29.8
[root@master ~]# kubectl drain node node2.example.com
Error from server (NotFound): nodes "node" not found
[root@master ~]# kubectl drain node2.example.com
[root@master ~]# kubectl drain node2.example.com
node/node2.example.com already cordoned
error: unable to drain node "node2.example.com" due to error:cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-flannel/kube-flannel-ds-kxpm4, kube-system/fluentd-elasticsearch-mdldz, kube-system/kube-proxy-28bgv, per/prometheus-prometheus-node-exporter-fpdz9, continuing command...
There are pending nodes to be drained:
 node2.example.com
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-flannel/kube-flannel-ds-kxpm4, kube-system/fluentd-elasticsearch-mdldz, kube-system/kube-proxy-28bgv, per/prometheus-prometheus-node-exporter-fpdz9
[root@master ~]# kubectl drain node2.example.com --ignore-daemonsets

[root@master ~]# kubectl drain node2.example.com --ignore-daemonsets
node/node2.example.com already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-kxpm4, kube-system/fluentd-elasticsearch-mdldz, kube-system/kube-proxy-28bgv, per/prometheus-prometheus-node-exporter-fpdz9
evicting pod classwork/resdeploy2-744758457d-pxm26
evicting pod classwork/resdeploy2-744758457d-nqd8r
pod/resdeploy2-744758457d-nqd8r evicted
pod/resdeploy2-744758457d-pxm26 evicted
node/node2.example.com drained
[root@master ~]# kubectl get nodes
NAME                 STATUS                     ROLES           AGE     VERSION
master.example.com   Ready                      control-plane   9d      v1.29.8
node1.example.com    Ready                      <none>          9d      v1.29.8
node2.example.com    Ready,SchedulingDisabled   <none>          5d11h   v1.29.8
[root@master ~]# kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE    IP             NODE                NOMINATED NODE   READINESS GATES
resdeploy2-744758457d-4msg9   1/1     Running   0          28s    192.168.0.16   node1.example.com   <none>           <none>
resdeploy2-744758457d-79vjl   1/1     Running   0          16h    192.168.0.13   node1.example.com   <none>           <none>
resdeploy2-744758457d-kzppd   1/1     Running   0          2m1s   192.168.0.14   node1.example.com   <none>           <none>
resdeploy2-744758457d-mg2k2   1/1     Running   0          28s    192.168.0.17   node1.example.com   <none>           <none>
resdeploy2-744758457d-p22mz   1/1     Running   0          16h    192.168.0.4    node1.example.com   <none>           <none>
resdeploy2-744758457d-tgms5   1/1     Running   0          2m     192.168.0.15   node1.example.com   <none>           <none>
[root@master ~]# 

    
    ===================
    
===========================================================================
Execute the below command in the new node.   
#hostnamectl set-hostname  minikube.example.com
#hostnamectl 


Minikube Install:
    
# dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
If you get any error says not able to resolv... then check your /etc/resolv.conf

#vim /etc/resolv.conf
search example.com
nameserver  8.8.8.8
-----save----

# dnf repolist
# dnf install docker-ce docker-ce-cli containerd.io -y
It throws some conflict issue.....
Known issue: 
#yum remove -y runc*  --> to remove runc* packages
 
 # dnf install docker-ce docker-ce-cli containerd.io -y
 
# systemctl enable docker
# systemctl start docker
# usermod -aG docker $USER
# newgrp docker
# curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
# cp kubectl /usr/local/bin/ && sudo chmod +x /usr/local/bin/kubectl
# kubectl version --client
# curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
# sudo install minikube-linux-amd64 /usr/local/bin/minikube
# minikube start --driver docker  --force
#minikube status
#kubectl cluster-info
# kubectl get nodes

# kubectl create deployment test-minikube --image=k8s.gcr.io/echoserver:1.10 
deployment.apps/test-minikube created
#
==========================================================================
Helm Charts Install:
   ======================================================== 
    https://helm.sh/docs/intro/install/
    Download and use the binary as per the above link
    #cd /root/Downloads
    #ls 
    helm.3.15......tar.gz
    #tar xvfz  helm-3.15.......tar.gz
    #cd linux-amd64
    #cp helm /usr/local/bin
    #helm
    ...======================================================
    
      $helm search repo bitnami
  $helm repo add bitnami https://charts.bitnami.com/bitnami


     $ helm repo update              

     

     $ helm list

 ==========================================No need to do the below excercise - only FYI =====
Installing Prometheus and followed by Grafanna:

What is prometheus?
https://prometheus.io/docs/introduction/overview/

Helm Charts:
    
    https://helm.sh/docs/intro/install/
    
      $helm search repo bitnami
  $helm repo add bitnami https://charts.bitnami.com/bitnami


     $ helm repo update              

     

     $ helm install bitnami/mysql --generate-name


       $ helm list



Helm Charts:
    
    https://helm.sh/docs/intro/install/
    
      $helm search repo bitnami
  $helm repo add bitnami https://charts.bitnami.com/bitnami


     $ helm repo update              

     

       $ helm list



=========================================Start Excercise from below=============================
Installing Prometheus:
    # helm repo add bitnami https://charts.bitnami.com/bitnami
    # helm repo list
    # helm search repo bitnami
Add helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

Update helm repo
helm repo update

Install helm
helm install prometheus prometheus-community/prometheus

kubectl get pod
kubectl get all
kubectl get pv
kubectl get pvc

Expose Prometheus Service
This is required to access prometheus-server using your browser.

#kubectl expose service prometheus-server --type=NodePort --target-port=9090 --name=prometheus-server-ext
#kubectl get svc

Using "minikube ip" and get the ip address and add the port using above "kubectl get svc" command 
Now prometheus UI will be opened
----==========================================================End=========================
Installing Grafanna using Helm Charts:
    
Add helm repo
helm repo add grafana https://grafana.github.io/helm-charts

Update helm repo
helm repo update

Install helm
helm install grafana grafana/grafana

Once instllation is completed, you need to execute the below command: to get the password for the admin user:
    
    #kubectl get secret --namespace default grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo 
    
Expose Grafana Service
#kubectl expose service grafana --type=NodePort --target-port=3000 --name=grafana-ext
#kubectl get svc

With "minikube ip " along with the port number (take it from "kubectl get svc"), using browser, get the UI

using "admin" user and with the password, login.
Goto "Data Source" - click and see the options.  Normally have multiple data sources.  Select Prometheus 
Where in "Prometheus server URL" section, enter the http://IPAddress:Port - which you have entered to create prometheus UI 
select Save and Test

Come back and see the "create first dashboard" - click --> just see the options.
But, click left side 4squares option  -> select -> Dashboard -> select "Create Dashboard"  -> then select "import dashboard" ->  

enter "3662" (this is ID number where pre-created template is available) -> as you click on the "Load" button, you can see prometheus is loaded. ...
Also on the bottom select "prometheus" on the prometheus tab...then click "Import"

Now you can able to see the Dashboards...

======================================================Below Not to work - just FYI ==========
prometheus-kube-state-metrics:

#kubectl get svc
..
kube-state-metrics...........................

#kubectl expose service prometheus-kube-state-metrics --type=NodePort --target-port=8080 --name=prometheus-kube-state-metrics-ext 
#kubectl get svc 
---
prometheus-kube-state-metrics-ext ..............:30245 (like one port will be there)...
Open browser, use minikube ip and use this port now

It shows as below:
    
    Kube Metrics
    .Metrics
    .helathz
    
    click Metrics,  you can find lot of metric informations....
    
    take any query and put it in prometheus in UI... 
    which can give us some text format.
    The same can be given in graphical format using Grafana - thats the speciality of grafana 



=========================================

27Aug24:
    
    Installing Ansible:
        
        #python3 --version
        #yum install ansible -y
        #ansible --version

PasswordLess Auth:
    
    Create user "ansible" in all the nodes
    master  ~#useradd ansible
    master ~# passwd ansible      -> note: set the password as "redhat"
    master ~#su - ansible
    master  ansible$ ssh-keygen
    Enter for 3 times...
     master  ansible$ ssh-copy-id ansible@node1  -> this should be running only in master node
     
     ============
      node1 #useradd ansible
      node1# passwd ansible      -> note: set the password as "redhat"
      node1 ~#su - ansible
     ansible$ ssh-keygen
     Enter for 3 times...
     node1  ansible$ 
     node1  ansible$ ssh-copy-id ansible@node1
     
    Continue above steps you have done in node1  for node2 also
    
    command to check the user which is created:
        cat /etc/passwd | grep <username>
    
    Results like below:
        [ansible@master ~]$ ssh ansible@node1
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Tue Aug 27 10:47:20 2024
[ansible@node1 ~]$ exit
logout
Connection to node1 closed.
[ansible@master ~]$ ssh ansible@node2

[ansible@node2 ~]$ exit
logout
Connection to node2 closed.


Sudo Privilege:
node1# cd /etc/sudoers.d
node1# vim ansible
ansible        ALL=(ALL)          NOPASSWD: ALL

---save---
Verification:
node1# su - ansible
node1  ansible$ sudo useradd user100
node1  ansible$ cat /etc/passwd | tail  -1     >> Just verify the user's creation

Similarly do the same for node2:
node2# cd /etc/sudoers.d
node2# vim ansible
ansible        ALL=(ALL)          NOPASSWD: ALL

---save---
Verification:
node2# su - ansible
node2  ansible$ sudo useradd user100
node2  ansible$ cat /etc/passwd | tail  -1     >> Just verify the user's creation
    
    =============

Create ansible.cfg file

master# su - ansible
[ansible@master ~]$ vim ansible.cfg
[defaults]
inventory=/home/ansible/myinventory2
remote_user=ansible

[privilege_escalation]
become=True
become_method=sudo
become_user=root
become_ask_pass=False
[ansible@master ~]$ 
-----save------------

Create the inventory

[ansible@master ~]$ vim myinventory2
node1.example.com
node2.example.com
-------save------------------

[ansible@master ~]$ ansible -m user -a 'name=user21 state=present' node1.example.com

-------------Now the user is created -----------

Playbook Creation:
    
    
[ansible@master ~]$ vim simpleplay.yml
- name: my first play
  hosts: node1.example.com
  tasks:
  - name: my first task
    user:
      name: user60
      state: present
  - name: my second task
    yum:
      name: httpd
      state: present
[ansible@master ~]$ 
[ansible@master ~]$ ansible-playbook simpleplay.yml  --syntax-check     ---> this will give you if any syntax errors available 
[ansible@master ~]$ ansible-playbook simpleplay.yml



------------------
Assessment on Wednesday:
Fist hour will be MCQ part
Next 3 hours will be Lab Assessment -  Dockers and Kubernetes questions will be there.   
Note: Ansible and Helm will be included only in MCQ part not in lab assessment. 

For evaluation purpose, you will need to capture the output and upload on LMS Portal  - as document or screenshots.
All the Best!!!



